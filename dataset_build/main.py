#!/home/gong0092/miniconda3/envs/gong_env/bin/python
# -*- coding: utf-8 -*-
"""
Refactored Bird Dataset Build Script - Main Application
Split from original bird_med_websearch_build_oop.py and adapted for Bird dataset
"""

import os
import sys
import json
import logging
import time
import argparse
import glob
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv
from functools import wraps
from concurrent.futures import ThreadPoolExecutor, as_completed

# Import external tools
from external_tools import (
    ExternalToolManager, 
    ExternalToolType,
    SubtaskResult
)

class GoldSubtaskManager:
    """Manages gold subtasks including SQL tools and external search tools"""
    
    def __init__(self, file_system_path: Optional[str] = None):
        # Pass file system path to external tool manager
        self.external_tool_manager = ExternalToolManager(file_system_path=file_system_path)
        self.logger = logging.getLogger(__name__)
    
    def get_gold_result_for_instance(self, instance_id: str, gold_result_dir: str) -> str:
        """Get gold result for a specific instance"""
        try:
            # Try exact match first
            gold_file_path = os.path.join(gold_result_dir, f"{instance_id}.csv")
            if os.path.exists(gold_file_path):
                with open(gold_file_path, "r", encoding="utf-8") as f:
                    return f.read()
            
            # Try with suffix patterns
            pattern = os.path.join(gold_result_dir, f"{instance_id}_*.csv")
            matching_files = glob.glob(pattern)
            
            if matching_files:
                matching_files.sort()
                with open(matching_files[0], "r", encoding="utf-8") as f:
                    return f.read()
            
            self.logger.warning(f"No gold result found for instance: {instance_id}")
            return "N/A"
            
        except Exception as e:
            self.logger.error(f"Error loading gold result for {instance_id}: {e}")
            return f"Error: {e}"
    
    def get_sql_for_instance(self, instance_id: str, sql_dir: str) -> str:
        """Get SQL statement for a specific instance"""
        try:
            # Try exact match first
            sql_file_path = os.path.join(sql_dir, f"{instance_id}.sql")
            if os.path.exists(sql_file_path):
                with open(sql_file_path, "r", encoding="utf-8") as f:
                    return f.read().strip()
            
            # Try with suffix patterns
            pattern = os.path.join(sql_dir, f"{instance_id}_*.sql")
            matching_files = glob.glob(pattern)
            
            if matching_files:
                matching_files.sort()
                with open(matching_files[0], "r", encoding="utf-8") as f:
                    return f.read().strip()
            
            self.logger.warning(f"No SQL found for instance: {instance_id}")
            return "N/A"
            
        except Exception as e:
            self.logger.error(f"Error loading SQL for {instance_id}: {e}")
            return f"Error: {e}"
    
    def execute_subtasks_concurrent(self, instance_id: str, original_query: str, 
                                   gold_result_dir: str, sql_dir: str) -> Dict[str, SubtaskResult]:
        """Execute all subtasks concurrently and return results"""
        self.logger.info("ğŸ“Š Gathering data from subtasks concurrently...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_gold = executor.submit(self.get_gold_result_for_instance, instance_id, gold_result_dir)
            future_sql = executor.submit(self.get_sql_for_instance, instance_id, sql_dir)
            future_external = executor.submit(self.external_tool_manager.search, original_query, original_query)
            
            futures = {
                'get_schema_info': None,  # Placeholder for schema info
                'gold_result': future_gold,
                'sql_statement': future_sql,
                'external_search': future_external
            }
            
            results = {}
            for future in as_completed([f for f in futures.values() if f is not None]):
                for name, fut in futures.items():
                    if fut == future:
                        try:
                            if name == 'external_search':
                                result = future.result()  # This is already a SubtaskResult
                                results[name] = result
                            else:
                                result = future.result()
                                results[name] = SubtaskResult(name, result, success=True)
                            
                            elapsed = time.time() - start_time
                            self.logger.info(f"âœ… {name} completed at {elapsed:.2f}s")
                        except Exception as e:
                            self.logger.error(f"âŒ {name} failed: {e}")
                            results[name] = SubtaskResult(name, f"Error: {e}", success=False, error=str(e))
                        break
        
        # Add schema info placeholder
        results['get_schema_info'] = SubtaskResult('get_schema_info', 'Schema info placeholder', success=True)
        
        end_time = time.time()
        self.logger.info(f"ğŸ“Š All subtasks completed: {end_time - start_time:.2f}s")
        
        return results
    
    def build_gold_subtasks(self, db: str, original_query: str, sql_statement: str, 
                           gold_result: str, query: str, selected_tool_type: str = None) -> List[Dict]:
        """Build gold subtasks following original script structure with dynamic external tool selection"""
        subtasks = [
            {
                "subtask_id": "get_schema_info",
                "tool": "get_schema_info",
                "input": {"database_name": db},
                "description": f"Provide schema information about the database"
            },
            {
                "subtask_id": "generated_sql",
                "tool": "generated_sql",
                "input": {
                    "natural_language_query": original_query,
                    "database_name": db
                },
                "expected_SQL": sql_statement,
                "description": f"Provide SQL to answer: {original_query}"  
            },
            {
                "subtask_id": "execute_sql",
                "tool": "execute_sql",
                "input": {
                    "database_name": db
                },
                "expected_result": gold_result,
                "description": f"Execute SQL to answer: {original_query}"  
            }
        ]
        
        # Add dynamic fourth subtask based on selected tool type
        if selected_tool_type == "VECTOR_SEARCH":
            subtasks.append({
                "subtask_id": "vectorDB_search",
                "tool": "vectorDB_search",
                "description": f"Retrieve relevant context for: {query}"
            })
        elif selected_tool_type == "FILE_SYSTEM":
            subtasks.append({
                "subtask_id": "file_system", 
                "tool": "file_system",
                "input": {
                    "natural_language_query": original_query
                },
                "description": f"Provide file information to answer: {query}"
            })
        else:  # Default to perplexity_search
            subtasks.append({
                "subtask_id": "web_context_search",
                "tool": "perplexity_search",
                "description": f"Retrieve relevant external context for: {query}"
            })
        
        return subtasks

class SingleChoiceGenerator:
    """Handles single choice question generation with human feedback support"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')
        self.logger = logging.getLogger(__name__)
        self.conversation_history = []
    
    def call_llm(self, prompt: str) -> str:
        """Simple LLM call using OpenRouter with improved error handling"""
        try:
            if not self.api_key:
                self.logger.warning("Missing OPENROUTER_API_KEY, returning empty response")
                return ""
                
            import requests
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                data=json.dumps({
                    "model": "anthropic/claude-3.5-sonnet",
                    "messages": [{"role": "user", "content": prompt}]
                })
            )
            
            if response.status_code == 200:
                response_data = response.json()
                if 'choices' in response_data and response_data['choices']:
                    return response_data['choices'][0]['message']['content'].strip()
        
            return ""
            
        except Exception as e:
            self.logger.error(f"LLM call error: {e}")
            return ""
    
    def generate_single_choice_content(self, original_query: str, subtask_results: Dict[str, SubtaskResult]) -> Dict:
        """Generate single choice questions using subtask results with random correct answer"""
        import random
        
        # Extract results from subtasks
        gold_result = subtask_results['gold_result'].result
        external_result = subtask_results['external_search'].result
        
        # Randomly select correct answer from all options
        correct_options = ['A', 'B', 'C', 'D']
        random_correct_answer = random.choice(correct_options)
        
        # Build prompt parts for single external source
        prompt_parts = [
            f"Original question: {original_query}(This part is not allowed to change. No deletions or Rephrase allowed)\n",
            f"Structured/statistical result: {gold_result}\n", 
            f"External knowledge: {external_result}\n",
            "\n"
        ]
        
        # Get prompt template with randomized correct answer
        unified_prompt = "".join(prompt_parts) + self._get_prompt_template(random_correct_answer)
        
        # Add to conversation history
        self.conversation_history.append({"role": "user", "content": unified_prompt})
        
        response = self.call_llm(unified_prompt)
        
        # Add response to conversation history
        self.conversation_history.append({"role": "assistant", "content": response})
        
        # Try to parse JSON response
        try:
            result = json.loads(response)
            questions = result.get("single_choice_questions", [])
            
            if questions and len(questions) > 0:
                question = questions[0]
                # Validate question structure
                if all(key in question for key in ["question", "options", "correct_answer", "explanation"]):
                    # Ensure correct answer matches what we requested
                    if question.get("correct_answer") != [random_correct_answer]:
                        self.logger.info(f"Correcting answer from {question.get('correct_answer')} to [{random_correct_answer}]")
                        question["correct_answer"] = [random_correct_answer]
                    
                    return {
                        "question_data": question,
                        "advanced_query": question.get("question", original_query),
                        "success": True,
                        "conversation_history": self.conversation_history.copy()
                    }
            
            # Fallback if parsing succeeds but structure is invalid
            return self._create_fallback_single_choice(original_query, gold_result, external_result, random_correct_answer)
            
        except json.JSONDecodeError as e:
            self.logger.warning(f"JSON parsing failed: {e}, using fallback")
            return self._create_fallback_single_choice(original_query, gold_result, external_result, random_correct_answer)
    
    def _get_prompt_template(self, correct_answer: str = 'B') -> str:
        """Get the prompt template for single choice generation with randomized correct answer"""
        return (
            "Based on the above information, generate a comprehensive response that includes:\n"
            "ONE single-choice question to test understanding \n\n"  
            "An advanced question that combines the original question with analytical extensions (Not a key factor in solving the problem)\n"

            "- Mathematical calculations don't have to be difficult (addition, subtraction, multiplication and division within 1000 are sufficient, and no complicated calculations are required), but they must be correct! ! ! !\n"


            "You must respond with valid JSON in this exact format:\n"
            "{\n"
            "  \"single_choice_questions\": [\n"
            "    {\n"
            "      \"question\": \"[original_query + Question that requires reasoning and understanding + Calculation rules have to be used. (based on original_query andreal world insightful situation of data analysis actual meaning, The Calculation rules of the inference calculation from gold_result to get the answer must be explained here! ! ! ! Do not use any calculations that are not mentioned)]\",\n"
            "      \"options\": {\n"
            "        \"A\": \"[First option text (The calculated value and practical strategic inspirational significance)]\",\n"
            "        \"B\": \"[Second option text (The calculated value and practical strategic inspirational significance)]\",\n"
            "        \"C\": \"[Third option text (The calculated value and practical strategic inspirational significance)]\",\n"
            "        \"D\": \"[Fourth option text (The calculated value and practical strategic inspirational significance)]\"\n"
            "      },\n"
            f"    \"correct_answer\": [\"{correct_answer}\"] (single-choice only - array with one correct option),\n"
            "      \"explanation\": \"[Explanation of why the answer is correct]\"\n"
            "    }\n"
            "  ]\n"
            

            "CRITICAL REQUIREMENTS for single_choice_questions:\n"
            "- You should guarantee that the question must be not able to be answered by the original question, and the answer must be answered correctly without the web and vector summary\n"
            "- EVERY question MUST begin with the complete Original question, No deletions or Rephrase allowed, then add inference elements\n"
            "- The question must not expose raw numerical values from gold_result or external sources. However, the correct answer must be derivable only by performing calculations using gold_result values, combined with relevant external knowledge.\n"
            "- Mathematical calculations don't have to be difficult (addition, subtraction, multiplication and division within 1000 are sufficient, and no complicated calculations are required), but they must be correct! ! ! \n"
            "- Error options must be obvious calculation error According to the same error gold_result reasoning. However the practical strategic inspirational significance should looks correct , Only calculation errors are used to judge\n"
            "- The value in any option MUST be calculated according to the calculation rules in the question. Do not use any calculation rules not mentioned in the question.!!! \n"
            "- All incorrect options must be derived from a consistent miscalculation pattern applied to the correct gold_result\n"
            f"- The correct answer must be placed in option {correct_answer}! Make sure option {correct_answer} contains the correct calculation and reasoning.\n"
            "- Questions MUST require combining numerical values from gold_result with external knowledge to determine the answer\n"
            "- Options must NEVER contain the exact numerical values from gold_result - this is ABSOLUTELY FORBIDDEN. But they must contain different values â€‹â€‹obtained by reasoning with the exact value in gold_result, which is also the key to the question - mathematics and reasoning ability\n"
            "- Each option must contain DIFFERENT mathematical transformations of gold_result values (e.g., percentages, Plus, minus)\n"
            "- All incorrect options must result from consistent miscalculations based on the same flawed logic applied to the correct gold_result\n"
            f"- Put the CORRECT calculation and reasoning in option {correct_answer}. Put calculation errors in the other options.\n"
            "- After each option's value, add the actual meaning of the value and the data analysis in the question scenario.\n"
            "- don't need to do complex calculations, but the calculation logic and practical significance must be reasonable.\n" 
            "- The relationship between each option and the gold_result must not be directly obvious; instead, it should require at least one explicit arithmetic step to derive, such as a percentage change, ratio, or sum\n"
            "- In the question, do not include specific values from external knowledge or gold_result\n"
            f"- In the explanation, clearly show the exact calculations that make option {correct_answer} correct, and explain why the other options are wrong due to calculation errors.\n"
            "- It should be IMPOSSIBLE to answer correctly without having the exact gold_result data and performing calculations\n"
            "- Each option must present a different mathematical transformation of the gold_result data\n"
            "- All mathematical transformations used in the options must be meaningful within the context of the questionâ€”reflecting realistic operations or metrics commonly used in that domain (e.g., percentage growth, average rate, total cost)\n"
            "- Every question must be an inference question requiring both gold_result data AND external knowledge\n"
            "- Questions must be SINGLE CHOICE ONLY - exactly one correct answer per question\n\n"

           
            "Additional Requirements for single_choice_questions:\n"
            "- Generate exactly 1 question\n"  
            "- Avoid revealing specific gold_results number in the question!!!\n\n"
            "- Design questions that require multi-step reasoning and cross-referencing between structured data, web insights, and vector database knowledge\n"
            "- Each question should test the agent's ability to synthesize information from multiple sources and perform analytical jumps\n"
            "- Create scenarios where agents must combine quantitative findings with qualitative external context to reach conclusions\n"
            "- Avoid simple fact recall, direct data lookup, or surface-level comprehension questions\n"
            "- Ensure the use of quantitative evidence in explanations is relevant, non-redundant, and directly supports the reasoning behind the answer.\n"
            "- Ensure questions challenge the agent to demonstrate deep analytical thinking and cross-domain knowledge integration\n"
            "- Questions must be SINGLE CHOICE ONLY - exactly one correct answer per question\n"
            "  Return only valid JSON, no other text or explanation."
        )
    
    def _create_fallback_single_choice(self, original_query: str, gold_result: str, external_result: str, correct_answer: str = 'B') -> Dict:
        """Create fallback single choice question when generation fails with random correct answer"""
        # Create options with the correct answer marked
        options = {
            "A": "Option A - Initial analysis approach",
            "B": "Option B - Alternative analytical approach",  
            "C": "Option C - Alternative interpretation",
            "D": "Option D - Incorrect methodology"
        }
        
        # Mark the correct option
        options[correct_answer] = options[correct_answer].replace("Alternative", "Correct") + " (Correct)"
        
        fallback_question = {
            "question": f"{original_query} Based on the data analysis, what can be inferred?",
            "options": options,
            "correct_answer": [correct_answer],
            "explanation": f"Fallback explanation - option {correct_answer} is correct as it requires combining database results with external context insights."
        }
        
        return {
            "question_data": fallback_question,
            "advanced_query": f"{original_query} (Enhanced with analytical extensions)",
            "success": False,
            "conversation_history": self.conversation_history.copy()
        }
    
    def revise_content(self, feedback: str, conversation_history: List[Dict], current_question_data: Dict, original_query: str, gold_result: str, external_result: str) -> Dict:
        """Revise content based on feedback using unified approach like content_generators.py"""
        # Build revision prompt following content_generators.py pattern
        revision_prompt = (
            f"Based on the feedback: {feedback}\n\n"
            "Please revise the previous response to address the feedback. "
            "Maintain the same JSON format. Use 'single_choice_questions' key. "
            "Improve all components based on the suggestions while keeping the structure requirements."
        )
        
        # Add feedback to conversation history following reference pattern  
        messages = conversation_history + [{"role": "user", "content": revision_prompt}]
        
        # Call LLM with full conversation history (like content_generators.py)
        response = self._call_llm_with_messages(messages)
        
        # Parse the revised response using safe parsing (like content_generators.py)
        parsed_json, error_msg = self._safe_json_parse(response, "revise_content")
        
        if parsed_json is not None:
            try:
                # Extract single choice questions
                single_choices = parsed_json.get("single_choice_questions", [])
                advanced_query = parsed_json.get("advanced_query", "")
                
                # Validate single choices following reference pattern
                validated_choices = self._validate_single_choices(single_choices)
                
                # Update conversation history
                updated_history = messages + [{"role": "assistant", "content": response}]
                
                # Build result following content_generators.py pattern
                result = {
                    "conversation_history": updated_history,
                    "success": True
                }
                
                if validated_choices:
                    question_data = validated_choices[0]  # Get first validated question
                    result.update({
                        "question_data": question_data,
                        "advanced_query": question_data.get("question", advanced_query or original_query),
                        "single_choice_questions": validated_choices
                    })
                else:
                    # Fallback if no valid choices
                    result.update({
                        "question_data": current_question_data,
                        "advanced_query": current_question_data.get("question", ""),
                        "success": False
                    })
                
                return result
                
            except Exception as e:
                self.logger.error(f"Error in revision: {e}")
                return self._create_revision_fallback(current_question_data, conversation_history, feedback)
        else:
            self.logger.warning(f"Failed to parse revised response: {error_msg}")
            return self._create_revision_fallback(current_question_data, conversation_history, feedback)
    
    def _call_llm_with_messages(self, messages: List[Dict]) -> str:
        """Call LLM with message history (adapted from content_generators.py)"""
        try:
            if not self.api_key:
                self.logger.warning("Missing OPENROUTER_API_KEY, returning empty response")
                return ""
                
            import requests
            
            # Convert messages to OpenAI format
            openai_messages = []
            for msg in messages:
                if isinstance(msg, dict):
                    if "role" in msg and "content" in msg:
                        openai_messages.append({"role": msg["role"], "content": msg["content"]})
                else:
                    # Handle HumanMessage-like objects
                    openai_messages.append({"role": "user", "content": str(msg)})
            
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                data=json.dumps({
                    "model": "anthropic/claude-3.5-sonnet",
                    "messages": openai_messages
                })
            )
            
            if response.status_code == 200:
                response_data = response.json()
                if 'choices' in response_data and response_data['choices']:
                    return response_data['choices'][0]['message']['content'].strip()
        
            return ""
            
        except Exception as e:
            self.logger.error(f"LLM call with messages error: {e}")
            return ""
    
    def _safe_json_parse(self, content: str, function_name: str = ""):
        """Safe JSON parsing with detailed error handling (from content_generators.py)"""
        try:
            # Clean response content
            content = content.strip()
            
            # Remove markdown code blocks
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # Try to parse JSON
            parsed_json = json.loads(content)
            return parsed_json, None
            
        except json.JSONDecodeError as e:
            error_msg = f"JSON parse error in {function_name}: {e}"
            self.logger.error(error_msg)
            return None, error_msg
        except Exception as e:
            error_msg = f"Other parsing error in {function_name}: {e}"
            self.logger.error(error_msg)
            return None, error_msg
    
    def _validate_single_choices(self, choices):
        """Validate and clean single choice questions (from content_generators.py)"""
        if not isinstance(choices, list):
            return [self._get_default_single_choice()]
        
        validated = []
        for choice in choices:
            if isinstance(choice, dict) and all(key in choice for key in ["question", "options", "correct_answer", "explanation"]):
                # Validate options structure
                options = choice.get("options", {})
                if isinstance(options, dict) and all(key in options for key in ["A", "B", "C", "D"]):
                    # Ensure single choice - correct_answer should have exactly one element
                    correct_answer = choice.get("correct_answer", [])
                    if isinstance(correct_answer, list) and len(correct_answer) == 1:
                        validated.append(choice)
                    else:
                        # Fix to single choice if multiple answers provided
                        fixed_choice = dict(choice)
                        if isinstance(correct_answer, list) and len(correct_answer) > 1:
                            fixed_choice["correct_answer"] = [correct_answer[0]]  # Take first answer
                        elif not isinstance(correct_answer, list):
                            fixed_choice["correct_answer"] = [str(correct_answer)]
                        else:
                            import random
                            default_options = ['A', 'B', 'C', 'D']
                            fixed_choice["correct_answer"] = [random.choice(default_options)]  # Random default single choice
                        validated.append(fixed_choice)
                else:
                    validated.append(self._get_default_single_choice())
            else:
                validated.append(self._get_default_single_choice())
        
        if len(validated) == 0:
            validated.append(self._get_default_single_choice())
        
        return validated[:1]  # Return only first question
    
    def _get_default_single_choice(self):
        """Get a default single choice structure with random correct answer"""
        import random
        
        # Randomly select correct answer from all options
        correct_options = ['A', 'B', 'C', 'D']
        random_correct_answer = random.choice(correct_options)
        
        options = {
            "A": "Option A",
            "B": "Option B",
            "C": "Option C", 
            "D": "Option D"
        }
        
        # Mark the correct option
        options[random_correct_answer] += " (Correct)"
        
        return {
            "question": "Default single choice question due to validation failure",
            "options": options,
            "correct_answer": [random_correct_answer],  # Random single choice
            "explanation": f"Default explanation due to validation failure - option {random_correct_answer} is the correct answer"
        }
    
    def _create_revision_fallback(self, current_question_data: Dict, conversation_history: List[Dict], feedback: str):
        """Create fallback result when revision fails"""
        updated_history = conversation_history + [
            {"role": "user", "content": f"User requested revision with feedback: {feedback}"},
            {"role": "assistant", "content": "Revision failed, keeping original content"}
        ]
        
        return {
            "question_data": current_question_data,
            "advanced_query": current_question_data.get("question", ""),
            "conversation_history": updated_history,
            "success": False
        }
    
    def reflect_on_content(self, question_data: Dict, original_query: str, gold_result: str, external_result: str) -> str:
        """Generate reflection on the quality of generated content"""
        reflection_prompt = (
            f"Please review the following single choice question for quality:\n\n"
            f"Original query: {original_query}\n"
            f"Generated question: {question_data.get('question', '')}\n"
            f"Options: {json.dumps(question_data.get('options', {}), indent=2)}\n"
            f"Correct answer: {question_data.get('correct_answer', [])}\n"
            f"Explanation: {question_data.get('explanation', '')}\n\n"
            "Evaluate based on these criteria:\n"
            "1. Accuracy (does it match the original intent and data?)\n"
            "2. Completeness (does it fully test understanding?)\n"
            "3. Clarity (is it clear and logical?)\n"
            "4. Difficulty (appropriate analytical challenge?)\n\n"
            "Respond in this exact format:\n"
            "Decision: [Acceptable/Needs Revision]\n"
            "Reason: [Your reasoning in 1-2 sentences]\n"
            "Suggestions: [Specific suggestions if Decision is 'Needs Revision', otherwise write 'None']"
        )
        
        return self.call_llm(reflection_prompt)

class DatasetEntry:
    """Represents a single dataset entry"""
    
    def __init__(self, instance_id: str, db: str, level: str = "medium", 
                 database_type: str = "bird"):
        self.instance_id = instance_id
        self.db = db
        self.level = level
        self.database_type = database_type
        self.timestamp = time.time()
    
    def to_dict(self, question_data: Dict, gold_subtasks: List[Dict]) -> Dict:
        """Convert to dictionary format following original script structure"""
        choice_question = question_data.get("question", "")
        
        return {
            "instance_id": self.instance_id,
            "db": self.db,
            "level": self.level,
            "database_type": self.database_type,
            "question_type": "single_choice",
            "tools_available": ["get_schema_info", "generated_sql", "execute_sql", "perplexity_search", "vectorDB_search", "sql_optimize", "file_system", "context_history", "sql_debug"],
            "gold_subtasks": gold_subtasks,  
            "query": choice_question,
            "options": question_data.get("options", {}),
            "correct_answer": question_data.get("correct_answer", []),
            "explanation": question_data.get("explanation", "")
        }

class BirdDatasetBuilder:
    """Main class for building Bird dataset with smart tool selection and human feedback"""
    
    def __init__(self, config: Dict[str, str]):
        self.config = config
        # Pass file system path from config
        file_system_path = config.get("file_system_path")
        self.subtask_manager = GoldSubtaskManager(file_system_path)
        self.question_generator = SingleChoiceGenerator()
        self.logger = self._setup_logging()
        
        # Human feedback settings
        self.interactive_mode = config.get('interactive', True)  # Default to True
        self.max_revisions = config.get('max_revisions', 3)
        
        # Statistics
        self.processed_count = 0
        self.error_count = 0
        self.accepted_count = 0
        self.revised_count = 0
        self.disposed_count = 0
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        log_path = self.config.get("log_path", "/tmp/bird_build.log")
        log_dir = os.path.dirname(log_path)
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)
        
        if root_logger.handlers:
            for handler in root_logger.handlers:
                root_logger.removeHandler(handler)
        
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
        
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        root_logger.addHandler(console_handler)
        
        return logging.getLogger(__name__)
    
    def display_results(self, original_query: str, content_result: Dict, subtask_results: Dict[str, SubtaskResult], reflection: str = ""):
        """Display generated content and results to user"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ GENERATED CONTENT REVIEW")
        print("=" * 80)
        
        # Show original query
        print(f"\nğŸ“ Original Query:")
        print(f"   {original_query}")
        
        # Show subtask results summary
        print(f"\nğŸ” Data Sources:")
        gold_result = subtask_results.get('gold_result', SubtaskResult('gold_result', 'N/A')).result
        external_result = subtask_results.get('external_search', SubtaskResult('external_search', 'N/A')).result
        
        print(f"   â€¢ Gold Result: {gold_result[:100]}{'...' if len(gold_result) > 100 else ''}")
        print(f"   â€¢ External Knowledge: {external_result[:100]}{'...' if len(external_result) > 100 else ''}")
        
        # Show generated question
        question_data = content_result.get("question_data", {})
        print(f"\nâ“ Generated Question:")
        print(f"   {question_data.get('question', 'N/A')}")
        
        # Show options
        print(f"\nğŸ“‹ Options:")
        options = question_data.get('options', {})
        for key in ['A', 'B', 'C', 'D']:
            if key in options:
                print(f"   {key}. {options[key]}")
        
        print(f"\nâœ… Correct Answer: {question_data.get('correct_answer', [])}")
        print(f"\nğŸ’¡ Explanation:")
        print(f"   {question_data.get('explanation', 'N/A')}")
        
        # Show reflection if available
        if reflection:
            print(f"\nğŸ¤” Quality Assessment:")
            print(f"   {reflection}")
        
        print("\n" + "=" * 80)
    
    def get_user_feedback(self) -> tuple:
        """Get user feedback and choice"""
        import sys
        
        # Check if we're in an interactive environment
        if not sys.stdin.isatty():
            # Non-interactive environment - auto-accept for demonstration
            self.logger.warning("âš ï¸ Non-interactive environment detected, auto-accepting content")
            print("\nâš ï¸ Non-interactive environment - auto-accepting content")
            return 'a', ""
        
        print("\nğŸ¯ Review Options:")
        print("   (a) Accept - Save this content and continue")
        print("   (d) Dispose - Skip this item completely") 
        print("   (r) Revise - Provide feedback for improvement")
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ Your choice (a/d/r): ").strip().lower()
                if user_input in ['a', 'd', 'r']:
                    feedback = ""
                    if user_input == 'r':
                        print("\nğŸ“ Please provide specific feedback for improvement:")
                        feedback = input("ğŸ’¬ Your feedback: ").strip()
                        if not feedback:
                            print("âŒ Feedback cannot be empty. Please try again.")
                            continue
                    return user_input, feedback
                else:
                    print("âŒ Invalid input. Please enter 'a', 'd', or 'r'.")
            except (EOFError, KeyboardInterrupt):
                # Handle non-interactive or interrupted input
                self.logger.warning("âš ï¸ Input interrupted or unavailable, auto-accepting content")
                print("\nâš ï¸ Input interrupted - auto-accepting content")
                return 'a', ""
    
    def load_queries(self, query_path: str) -> List[Dict[str, Any]]:
        """Load queries from a JSONL file"""
        with open(query_path, "r", encoding="utf-8") as f:
            queries = []
            line_num = 0
            for line in f:
                line_num += 1
                if line.strip():
                    try:
                        data = json.loads(line)
                        queries.append(data)
                    except json.JSONDecodeError as e:
                        self.logger.error(f"JSON parse error in file {query_path} line {line_num}: {e}")
                        continue
            return queries
    
    def time_monitor(self, func_name=""):
        """Time monitoring decorator with logging"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    end_time = time.time()
                    elapsed = end_time - start_time
                    self.logger.info(f"â±ï¸  {func_name or func.__name__}: {elapsed:.2f}s")
                    return result
                except Exception as e:
                    end_time = time.time()
                    elapsed = end_time - start_time
                    self.logger.error(f"âŒ {func_name or func.__name__}: {elapsed:.2f}s (failed: {e})")
                    raise
            return wrapper
        return decorator
    
    def process_single_query(self, item: Dict, idx: int) -> bool:
        """Process a single query with human feedback support"""
        instance_id = item.get("instance_id") or item.get("id") or str(idx)
        db = item.get("db_id") or item.get("db") or "SQLite"
        original_query = item.get("instruction") or item.get("question") or item.get("query") or ""
        
        self.logger.info(f"ğŸ”„ Processing query {idx+1}: {instance_id}")
        total_start_time = time.time()
        
        try:
            # Execute all subtasks (smart tool selection)
            subtask_results = self.subtask_manager.execute_subtasks_concurrent(
                instance_id, original_query, self.config["gold_result_dir"], self.config["sql_path"]
            )
            
            if self.interactive_mode:
                return self._process_interactive_mode(item, idx, subtask_results, total_start_time)
            else:
                return self._process_auto_mode(item, idx, subtask_results, total_start_time)
            
        except Exception as e:
            total_end_time = time.time()
            self.logger.error(f"âŒ Error processing query {idx+1}: {e}")
            self.logger.error(f"ğŸ¯ Total processing time: {total_end_time - total_start_time:.2f}s")
            self.logger.error("=" * 60)
            import traceback
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            self.error_count += 1
            return False
    
    def _process_interactive_mode(self, item: Dict, idx: int, subtask_results: Dict[str, SubtaskResult], start_time: float) -> bool:
        """Process query in interactive mode with human feedback"""
        instance_id = item.get("instance_id") or item.get("id") or str(idx)
        db = item.get("db_id") or item.get("db") or "SQLite"
        original_query = item.get("instruction") or item.get("question") or item.get("query") or ""
        
        revision_count = 0
        content_result = None
        
        while revision_count <= self.max_revisions:
            # Generate single choice content only on first iteration
            if revision_count == 0:
                content_result = self.question_generator.generate_single_choice_content(
                    original_query, subtask_results
                )
            
            # Generate reflection
            if content_result["success"] and content_result.get("question_data"):
                reflection = self.question_generator.reflect_on_content(
                    content_result["question_data"],
                    original_query,
                    subtask_results['gold_result'].result,
                    subtask_results['external_search'].result
                )
            else:
                reflection = "Content generation failed or used fallback."
            
            # Display results to user
            self.display_results(original_query, content_result, subtask_results, reflection)
            
            # Get user feedback
            user_choice, feedback = self.get_user_feedback()
            
            if user_choice == 'a':
                # Accept and save
                self.accepted_count += 1
                return self._save_entry(item, content_result, subtask_results, start_time)
                
            elif user_choice == 'd':
                # Dispose - skip this item
                self.disposed_count += 1
                total_end_time = time.time()
                self.logger.info(f"ğŸ—‘ï¸  Query {idx+1} disposed by user")
                self.logger.info(f"ğŸ¯ Total processing time: {total_end_time - start_time:.2f}s")
                print(f"\nğŸ—‘ï¸  Query {idx+1} disposed and skipped.\n")
                return True
                
            elif user_choice == 'r':
                # Revise based on feedback
                if revision_count >= self.max_revisions:
                    print(f"\nâš ï¸  Maximum revisions ({self.max_revisions}) reached. Accepting current version.\n")
                    self.accepted_count += 1
                    return self._save_entry(item, content_result, subtask_results, start_time)
                
                print(f"\nğŸ”„ Revising content (attempt {revision_count + 1}/{self.max_revisions})...")
                
                # Get conversation history and current data
                conversation_history = content_result.get("conversation_history", [])
                current_question_data = content_result.get("question_data", {})
                
                # Get data sources for context
                gold_result = subtask_results['gold_result'].result
                external_result = subtask_results['external_search'].result
                
                # Attempt revision with full context
                revised_result = self.question_generator.revise_content(
                    feedback, conversation_history, current_question_data, 
                    original_query, gold_result, external_result
                )
                
                if revised_result["success"]:
                    content_result = revised_result
                    self.revised_count += 1
                    print("âœ… Content revised successfully!")
                else:
                    print("âŒ Revision failed, keeping original content.")
                
                revision_count += 1
        
        # If we exit the loop without accepting/disposing, save the last version
        self.accepted_count += 1
        return self._save_entry(item, content_result, subtask_results, start_time)
    
    def _process_auto_mode(self, item: Dict, idx: int, subtask_results: Dict[str, SubtaskResult], start_time: float) -> bool:
        """Process query in automatic mode without human feedback"""
        # Generate single choice content
        content_result = self.question_generator.generate_single_choice_content(
            item.get("instruction") or item.get("question") or item.get("query") or "", subtask_results
        )
        
        # Log generated question for debugging
        if content_result["success"]:
            self.logger.info(f"âœ… Generated single choice question for {item.get('instance_id', idx)}")
        else:
            self.logger.warning(f"âš ï¸ Used fallback question for {item.get('instance_id', idx)}")
        
        self.accepted_count += 1
        return self._save_entry(item, content_result, subtask_results, start_time)
    
    def _save_entry(self, item: Dict, content_result: Dict, subtask_results: Dict[str, SubtaskResult], start_time: float) -> bool:
        """Save the final entry to output"""
        try:
            instance_id = item.get("instance_id") or item.get("id") or "unknown"
            db = item.get("db_id") or item.get("db") or "SQLite"
            original_query = item.get("instruction") or item.get("question") or item.get("query") or ""
            
            # Create dataset entry
            dataset_entry = DatasetEntry(instance_id, db)
            # Get selected tool type from external search result
            selected_tool_type = getattr(subtask_results['external_search'], 'selected_tool_type', None)
            
            gold_subtasks = self.subtask_manager.build_gold_subtasks(
                db, original_query, 
                subtask_results['sql_statement'].result,
                subtask_results['gold_result'].result, 
                content_result["question_data"].get("question", ""),
                selected_tool_type
            )
            
            entry_dict = dataset_entry.to_dict(content_result["question_data"], gold_subtasks)
            
            # Save to output
            self._append_to_output(entry_dict)
            
            total_end_time = time.time()
            self.logger.info(f"âœ… Successfully processed and saved entry")
            self.logger.info(f"ğŸ¯ Total processing time: {total_end_time - start_time:.2f}s")
            
            if self.interactive_mode:
                print(f"\nğŸ’¾ Entry saved successfully!\n")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Error saving entry: {e}")
            return False
    
    def _append_to_output(self, entry: Dict):
        """Append entry to output file"""
        output_path = self.config["output_path"]
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    def build_dataset(self):
        """Main method to build the dataset"""
        mode_text = "Interactive Human Review" if self.interactive_mode else "Automatic"
        self.logger.info(f"ğŸš€ Starting Bird Medium Single Choice Dataset Build ({mode_text} Mode)")
        self.logger.info(f"ğŸ“ Output path: {self.config['output_path']}")
        self.logger.info("ğŸ§  Using LLM-based smart tool selection")
        
        if self.interactive_mode:
            self.logger.info(f"ğŸ‘¤ Interactive mode enabled - Human review required for each item")
            self.logger.info(f"ğŸ”„ Maximum revisions per item: {self.max_revisions}")
            print(f"\nğŸ¯ Welcome to Interactive Dataset Building!")
            print(f"   You will review each generated question and can:")
            print(f"   â€¢ Accept good questions")
            print(f"   â€¢ Request revisions with specific feedback")
            print(f"   â€¢ Dispose of unsuitable items")
            print(f"   Maximum {self.max_revisions} revisions per item.\n")
        
        overall_start_time = time.time()
        
        # Load queries
        self.logger.info(f"ğŸ“‚ Loading queries from: {self.config['bird_path']}")
        
        if not os.path.exists(self.config["bird_path"]):
            self.logger.error(f"âŒ Bird path does not exist: {self.config['bird_path']}")
            return
        
        queries = self.load_queries(self.config["bird_path"])
        self.logger.info(f"ğŸ“Š Loaded {len(queries)} queries")
        
        # Process queries
        for idx, item in enumerate(queries):
            if self.process_single_query(item, idx):
                self.processed_count += 1
            else:
                self.error_count += 1
        
        overall_end_time = time.time()
        total_time = overall_end_time - overall_start_time
        
        # Display comprehensive statistics
        self.logger.info("=" * 80)
        self.logger.info("ğŸ BIRD DATASET BUILD COMPLETED")
        self.logger.info(f"ğŸ“Š Total queries loaded: {len(queries)}")
        self.logger.info(f"âœ… Successfully processed: {self.processed_count}")
        self.logger.info(f"âŒ Errors encountered: {self.error_count}")
        
        if self.interactive_mode:
            self.logger.info("=" * 50)
            self.logger.info("ğŸ‘¤ Human Review Statistics:")
            self.logger.info(f"   âœ… Accepted: {self.accepted_count}")
            self.logger.info(f"   ğŸ”„ Revised: {self.revised_count}")
            self.logger.info(f"   ğŸ—‘ï¸  Disposed: {self.disposed_count}")
            if self.processed_count > 0:
                acceptance_rate = (self.accepted_count / self.processed_count) * 100
                self.logger.info(f"   ğŸ“ˆ Acceptance rate: {acceptance_rate:.1f}%")
        
        self.logger.info(f"â±ï¸  Total time: {total_time:.2f}s")
        if self.processed_count > 0:
            avg_time = total_time / self.processed_count
            self.logger.info(f"âš¡ Average time per query: {avg_time:.2f}s")
        self.logger.info(f"ğŸ’¾ Output saved to: {self.config['output_path']}")
        self.logger.info("ğŸ§  Used intelligent LLM-based tool selection")
        self.logger.info("=" * 80)

def setup_environment():
    """Setup environment variables"""
    load_dotenv()

def main():
    """Main function"""
    # Setup environment
    setup_environment()
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Bird Smart Tool Selection Dataset Build Script with Human Review')
    parser.add_argument('--auto', action='store_true', help='Run in automatic mode without human review (default is interactive)')
    parser.add_argument('--max-revisions', type=int, default=3, help='Maximum number of revisions per item (default: 3)')
    args = parser.parse_args()
    
    # Configuration - using relative paths
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config = {
        "bird_path": os.path.join(current_dir, "input_data", "original_data", "bird.jsonl"),
        "gold_result_dir": os.path.join(current_dir, "input_data", "gold_sql_result"),
        "sql_path": os.path.join(current_dir, "input_data", "gold_sql_query"),
        "output_path": os.path.join(current_dir, "output_data", "medium_singlechoice.json"),
        "log_path": os.path.join(current_dir, "log", "bird_singlechoice.log"),
        "file_system_path": os.path.join(current_dir, "input_data", "file_system"),
        "database_type": "bird",
        "use_smart_tool_selection": True,  # Use smart tool selection for external search
        "interactive": not args.auto,  # Default to interactive mode (True) unless --auto is specified
        "max_revisions": args.max_revisions
    }
    
    # Create and run dataset builder
    builder = BirdDatasetBuilder(config)
    builder.build_dataset()

if __name__ == "__main__":
    main()